## Introduction

We have data, collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. One of values identifying the quality of the activity.
Testing dataset doesn't have labels of activity quality. We need to predict quality of the activity using trayning dataset.

## Getting and cleaning data

I decided to use caret package. Loading data from datasets:

```{r}
library(caret)

set.seed(9988)

train_path <- "pml-training.csv"
test_path <- "pml-testing.csv"
f_train <- read.csv(train_path, na.strings = c("NA", ""))
f_test <- read.csv(test_path, na.strings = c("NA", ""))
```

To estimate out-of-sample error, I splited randomly training dataset to 75% to training set, and 25% to testing set.

```{r}
inTrain <- createDataPartition(y = f_train$classe, p = 0.75, list = FALSE)
p_train <- f_train[inTrain, ]
p_test <- f_train[-inTrain, ]
```

To reduce number of features, I removed variables with nearly zero variance, having a lot of NA, and variables that don't have effect to prediction.

```{r}
nzv <- nearZeroVar(p_train)
p_train <- p_train[, -nzv]
p_test <- p_test[, -nzv]

NA_mark <- sapply(p_train, function(x) mean(is.na(x))) > 0.95
p_train <- p_train[, NA_mark == FALSE]
p_test <- p_test[, NA_mark == FALSE]

p_train <- p_train[, -(1:5)]
p_test <- p_test[, -(1:5)]
```

## Building model
First I decided try to use the Random Forest model, using 3-fold cross-validation.

```{r}
fitControl <- trainControl(method = "cv", number = 3, verboseIter = FALSE)
modRF1 <- train(classe ~ ., data = p_train, method = "rf", trControl = fitControl)
modRF1$finalModel
```


I saw that out-of-sample error is 0.22%, it means that accuracity of this model is 99.78%.
Looks good, and I decided to use this model to predict testing set values without changing any model's parameters.

Try to apply this model to my testing set:
```{r}
predictions <- predict(modRF1, p_test)
confusionMatrix(p_test$classe, predictions)
```

Accuracity on testing set is 99.65%. Very good result, and I can use this model to predict classes on original testing set of 20 cases.

## Chekup model with the Testing Set

Now I am using all of the training set to build prediction model. Preparing training and testing sets.
```{r}
nzv <- nearZeroVar(f_train)
f_train <- f_train[, -nzv]
f_test <- f_test[, -nzv]

NA_mark <- sapply(f_train, function(x) mean(is.na(x))) > 0.95
f_train <- f_train[, NA_mark == FALSE]
f_test <- f_test[, NA_mark == FALSE]
f_train <- f_train[, -(1:5)]
f_test <- f_test[, -(1:5)]
```
Building model and predicting classes:
```{r}
modRF2 <- train(classe ~ ., data = f_train, method = "rf", trControl = fitControl)
modRF2$finalModel
predictions <- predict(modRF2, f_test)
predictions
```

I uploaded this values to quiz of the course project, and got 20 of 20 points. It means that my model predicted 100% of values correctly. 
