## Introduction

We have the data, collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. One of values is identifying the quality of the activity.
The testing dataset doesn't have labels of activity quality. We need to predict quality of the activity using training dataset.

## Getting and cleaning data

Loading data from datasets:

```{r}
library(caret)

set.seed(9988)

train_path <- "pml-training.csv"
test_path <- "pml-testing.csv"
f_train <- read.csv(train_path, na.strings = c("NA", ""))
f_test <- read.csv(test_path, na.strings = c("NA", ""))
```

To estimate out-of-sample error, I splited the training dataset randomly to 75% to the new training set, and 25% to the testing set.

```{r}
inTrain <- createDataPartition(y = f_train$classe, p = 0.75, list = FALSE)
p_train <- f_train[inTrain, ]
p_test <- f_train[-inTrain, ]
```

To reduce number of features, I removed variables with nearly zero variance, having a lot of NA, and variables that don't have effect to prediction.

```{r}
nzv <- nearZeroVar(p_train)
p_train <- p_train[, -nzv]
p_test <- p_test[, -nzv]

NA_mark <- sapply(p_train, function(x) mean(is.na(x))) > 0.95
p_train <- p_train[, NA_mark == FALSE]
p_test <- p_test[, NA_mark == FALSE]

p_train <- p_train[, -(1:5)]
p_test <- p_test[, -(1:5)]
```

## Building a model
First I decided try to use the Random Forest model, using 3-fold cross-validation.

```{r}
fitControl <- trainControl(method = "cv", number = 3, verboseIter = FALSE)
modRF1 <- train(classe ~ ., data = p_train, method = "rf", trControl = fitControl)
modRF1$finalModel
```


I found that out-of-sample error was 0.22%, it means that accuracy of this model is 99.78%.
Looks good, and I decided to use this model to predict testing set values without tuning any model's parameters.

Applying this model to my testing set:
```{r}
predictions <- predict(modRF1, p_test)
confusionMatrix(p_test$classe, predictions)
```

Accuracy on testing dataset is 99.65%. It is a great result, and I can use this model to predict classes on original testing dataset of 20 cases.

## Chekup model with the Testing Set

Now I am using all of the training set to build prediction model. I prepare training and testing datasets.
```{r}
nzv <- nearZeroVar(f_train)
f_train <- f_train[, -nzv]
f_test <- f_test[, -nzv]

NA_mark <- sapply(f_train, function(x) mean(is.na(x))) > 0.95
f_train <- f_train[, NA_mark == FALSE]
f_test <- f_test[, NA_mark == FALSE]
f_train <- f_train[, -(1:5)]
f_test <- f_test[, -(1:5)]
```
I build the model and predict classes:
```{r}
modRF2 <- train(classe ~ ., data = f_train, method = "rf", trControl = fitControl)
modRF2$finalModel
predictions <- predict(modRF2, f_test)
predictions
```

I uploaded this values to quiz of the course project, and got 20 of 20 points. It means that my model predicted 100% of values correctly. 
