## Introduction

In this project I have data collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. Data has label that identifying the quality of the activity.
Testing dataset doesn't have labels of activity quality. I have to mark testing dataset with activity labels using trayning dataset.

## Getting and cleaning data

To make this work, I decided to use caret package. Then I load data from datasets.

```{r}
library(caret)

set.seed(9988)

train_path <- "pml-training.csv"
test_path <- "pml-testing.csv"
f_train <- read.csv(train_path, na.strings = c("NA", ""))
f_test <- read.csv(test_path, na.strings = c("NA", ""))
```

To estimate out-of-sample error, I split randomly training dataset to 75% to training set, and 25% to testing set.

```{r}
inTrain <- createDataPartition(y = f_train$classe, p = 0.75, list = FALSE)
p_train <- f_train[inTrain, ]
p_test <- f_train[-inTrain, ]
```

Then I need to reduce number of features. I removed variables with nearly zero variance, that are have a lot of NA, and variables that I guess don't have effect to prediction.

```{r}
nzv <- nearZeroVar(p_train)
p_train <- p_train[, -nzv]
p_test <- p_test[, -nzv]

NA_mark <- sapply(p_train, function(x) mean(is.na(x))) > 0.95
p_train <- p_train[, NA_mark == FALSE]
p_test <- p_test[, NA_mark == FALSE]

p_train <- p_train[, -(1:5)]
p_test <- p_test[, -(1:5)]
```

## Building model
First I decided try to use the Random Forest model, using 3-fold cross-validation.

```{r}
fitControl <- trainControl(method = "cv", number = 3, verboseIter = FALSE)
modRF1 <- train(classe ~ ., data = p_train, method = "rf", trControl = fitControl)
modRF1$finalModel
```

Result of building:
```
Call:
 randomForest(x = x, y = y, mtry = param$mtry) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 27

        OOB estimate of  error rate: 0.22%
Confusion matrix:
     A    B    C    D    E  class.error
A 4183    1    0    0    1 0.0004778973
B    9 2835    3    1    0 0.0045646067
C    0    6 2561    0    0 0.0023373588
D    0    0    9 2403    0 0.0037313433
E    0    1    0    2 2703 0.0011086475
```
I saw that out-of-sample error is 0.22%, it means that accuracity of this model is 99.78%.
Looks good, and I decided to use this model to predict testing set values without changing any model's parameters.

Try to apply this model to my testing set:
```{r}
predictions <- predict(modRF1, p_test)
confusionMatrix(p_test$classe, predictions)
```
Result:
```
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1395    0    0    0    0
         B    3  945    1    0    0
         C    0    6  848    1    0
         D    0    0    5  798    1
         E    0    0    0    0  901

Overall Statistics
                                         
               Accuracy : 0.9965         
                 95% CI : (0.9945, 0.998)
    No Information Rate : 0.2851         
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.9956         
 Mcnemar's Test P-Value : NA             

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9979   0.9937   0.9930   0.9987   0.9989
Specificity            1.0000   0.9990   0.9983   0.9985   1.0000
Pos Pred Value         1.0000   0.9958   0.9918   0.9925   1.0000
Neg Pred Value         0.9991   0.9985   0.9985   0.9998   0.9998
Prevalence             0.2851   0.1939   0.1741   0.1629   0.1839
Detection Rate         0.2845   0.1927   0.1729   0.1627   0.1837
Detection Prevalence   0.2845   0.1935   0.1743   0.1639   0.1837
Balanced Accuracy      0.9989   0.9963   0.9956   0.9986   0.9994
```

Accuracity on testing set is 99.65%. Very good result, and I can use this model to predict classes on original testing set of 20 cases.

## Work with Testing Set

Now I use all of training set to build prediction model. Prepare training and testing sets.
```{r}
nzv <- nearZeroVar(training)
f_train <- f_train[, -nzv]
f_test <- f_test[, -nzv]

NA_mark <- sapply(f_train, function(x) mean(is.na(x))) > 0.95
f_train <- f_train[, NA_mark == FALSE]
f_test <- f_test[, NA_mark == FALSE]
f_train <- f_train[, -(1:5)]
f_test <- f_test[, -(1:5)]
```
Build model and predict classes:
```{r}
modRF2 <- train(classe ~ ., data = f_train, method = "rf", trControl = fitControl)
modRF2$finalModel
predictions <- predict(modRF2, f_test)
```
Result:
```
> predictions
 [1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
```

I uploaded this values to quiz of the course project, and got 20 of 20 points. It means that my model predicted 100% of values correctly. 
